\section{Processamento/Tratamento de Dados}

Nem sempre utilizamos o texto inteiro quando queremos extrair conhecimento do seu conteúdo, que no nosso caso significa encontrar tópicos com suas palavras e 
encontrar documentos em que estes tópicos estão presentes. Muitas palavras não são interessantes, como por exemplo aquelas que não agregam valor 
mas influenciam o treinamento de modelos, como as conhecidas \textit{stopwords}. Estas podem, inclusive, atrapalhar o processo de aprendizado 
ao adicionar ruído ao conjunto de dados e aumentar o custo computacional e o tempo de execução.

A limpeza de texto consiste então de remover dos nossos documentos originais este conjunto de palavras que não somente não agregam ao treinamento como atrapalham 
o seu funcionamento adequado. Parte das tarefas de limpeza é comum a todas as fontes de dados e parte é específica por fonte de dados escolhida.

Estas atividades são na essência uma tentativa de conversão de linguagem de texto para algo mais próximo do que o computador consegue entender melhor,
preparando os dados de textos para o processamento de linguagem natural.

\subsection{Limpeza comum}

Algumas tarefas de limpeza de texto são úteis e devem ser empregadas a qualquer fonte de dados, mesmo que com parâmetros diferentes.

\begin{itemize}
    \item Remoção de tags HTML: são apenas marcadores para definir a formatação de conteúdo html e não devem fazer parte dos dados de treinamento ou teste
    \item Remoção de pontuações: servem apenas para definir a estrutura do texto e não devem ser utilizados no treinamento
    \item Conversão para minúsculo: todo o conteúdo deve ser normalizado convertendo as letras para minúsculas de forma que palavras escritas com 
    \textit{case} diferente não sejam tratadas como entidades distintas
    \item Remoção de \textit{stopwords}: contempla a remoção de \textit{stopwords} gerais da linguagem dos textos, em nosso caso português. Esta lista é 
    obtida de um pacote de processamento de linguagem natural
    \item Remoção de palavras adicionais: remover do texto palavras que não são \textit{stopwords} mas atrapalham o treinamento de alguma forma. 
    É uma questão deve ser avaliada especificamente para cada fonte de dados
    adicionais é diferente para cada fonte de dados. Esta lista é obtida após análise de exploração e conhecimento da fonte de dados 
    \item \textit{Stemming}: redução de palavras para sua forma raiz, necessário para uma mesma palavra que seja escrita de formas distintas seja 
    tratada como apenas uma entidade. Além disso este tratamento diminui a dimensão do conjunto total de palavras e reduz custo computacional.
    Como exemplo podemos citar as palavras \textit{visitei} e \textit{visita} que seriam reduzidas a \textit{visit}
\end{itemize}

Para exemplificar a execução de limpeza de texto vamos usar a seguinte frase: 

\textit{"Após visitar a Lagoa Dourada nós decidimos voltar o hotel. Foi um dia muito bonito, esta foi uma das lagoas mais bonitas que já visitamos."}

Após a limpeza temos como resultado a seguinte lista de palavras: 

\textit{['após', 'visit', 'lago', 'dour', 'decid', 'volt', 'hotel', 'foi', 'dia', 'bonit', 'lago', 'bonit', 'visit']}

Importante perceber alguns alterações importantes em relação ao texto inicial:

\begin{itemize}
    \item Conversão da palavra \textit{Após} para \textit{após} com o \textit{a} minúsculo
    \item Remoção de \textit{stopwords} como \textit{a, nós, o, um, esta, uma, das, mais, que, já}
    \item Alteração de \textit{case} e conversão da palavra \textit{Lagoa}, desta forma as duas referências a \textit{lagoa} que estavam escritas de 
    forma diferente agora são tratados como uma mesma entidades
    \item Remoção de pontuações
\end{itemize}

\subsection{Posts Nerds Viajantes}

\subsubsection{Limpeza de palavras nos posts}

Além da limpeza de texto padrão, foi necessário eliminar outros conjuntos de palavras dos posts do blog. Após execução da análise dos tópicos nas primeiras 
execuções de treinamento do modelo foi identificado que alguns conjuntos de palavras não fazem sentido ser usados na definição do dicionário.

\begin{itemize}
    \item Remoção de \textit{caption}: A ferramenta \textit{Wordpress} utilizada uma notação específica para adicionar legendas às imagens, que não corresponde ao padrão html e por isto foi necessária uma limpeza específica deste conteúdo
    \item Remoção de \textit{stopwords} específicas: Palavras que não fazem parte da lista de \textit{stopwords} da linguagem mas que mesmo assim 
    atrapalham no treinamento do modelo
    \begin{itemize}
        \item Exemplos: fot, fic, visit, fotograf, dia, algum, par, passei, bem, restaurant
        \item A lista completa destas palavras se encontra no arquivo \href{https://github.com/heldergr/tcc-pucmg-2/blob/main/src/python/notebooks/data/remocao_nerds_viajantes.txt}{remocao\_nerds\_viajantes.txt} no repositório do projeto.
    \end{itemize}
    \item Remoção de verbos: A ideia da comparação dos documentos foi mais direcionada a características dos locais visitados e decidi remover os verbos do texto pois estes contribuiam muito na composição dos tópicos
\end{itemize}

\subsubsection{Limpeza de posts inteiros}

Além da limpeza de palavras dentro dos posts, foi necessário também remover alguns posts do conjunto a ser utilizado no treinamento.

Nosso blog também tem um pouco de foco em fotografia e por muito tempo disponibilizamos \textbf{papéis de parede} de fotos que nós tiramos e gostamos. 
Para divulgar os papéis de parede escrevemos vários posts e estes não são interessantes para o propósito deste trabalho, por isso resolvi removê-los.
A remoção foi relativamente fácil, visto que todos os posts deste conjunto tem o atributo \textit{name} começando com o prefixo \textit{papel-de-parede}.

Outro critério para remoção de posts inteiros foi o \textbf{tamanho do documento em tokens}. Em várias referências sobre o algoritmo de identificação 
de tópicos LDA, é recomendado utilizar documentos cuja quantidade de tokens seja acima de 40 ou 50. Analisando os tamanhos posts do blog eu notei 
que 42 tokens seria um bom limite e este passou a ser o tamanho a partir do qual os documentos seriam usados no treinamento.

Além dos dois conjuntos acima eu também optei por analisar manual e visualmente o conjunto de posts para remover pontualmente alguns cujo conteúdo 
não seria interessante no treinamento. Páginas na ferramenta \textit{Wordpress} são consideradas posts e a página de contato, por exemplo, 
não interessa no nosso contexto. Posts com conteúdo publicitário também foram removidos. Posts com conteúdo de valor temporal (divulgação de um 
evento, por exemplo) que não tem mais valor atualmente também foram removidos. A lista completa pode ser consultada no 
\href{https://github.com/heldergr/tcc-pucmg-2/blob/main/src/python/notebooks/limpeza/limpeza_posts.py}{módulo de limpeza de posts}.

\subsection{Wikipedia}

No caso dos posts do blog a limpeza de tags html em posts é feita no carregamento dos dados antes do treinamento, mas no caso das páginas da 
Wikipedia eu resolvi fazer esta limpeza apenas uma vez e gravar em um atributo separado, aproveitando as facilidades do MongoDB para alteração 
na estrutura de documentos.

Ao fazer um teste com cerca de duas mil páginas o tempo para remover as tags foi mais de um minuto e como seria um trabalho a ser executado a 
cada treinamento eu achei melhor já fazer este processo previamente.

O campo utilizado com base para a limpeza foi o \textit{text}, que contém o conteúdo em html. Neste campo o conteúdo da página fica dentro de 
parágrafos (atributo html \textit{p}) e elementos pré-formatados (atributo html \textit{pre}).

Para cada texto eu utilizei o componente \textit{BeautifulSoup} do módulo \textit{bs4} para extrair todos os parágrafos e pré-formatados para, 
em seguida, gravá-los em um campo separada chamado \textit{text\_clean}, que foi usado no identificação de documentos semelhantes.

O código fonte completo pode ser encontrado no 
\href{https://github.com/heldergr/tcc-pucmg-2/blob/main/src/python/notebooks/limpeza/limpeza_posts.py}{módulo de limpeza da Wikipedia}.
