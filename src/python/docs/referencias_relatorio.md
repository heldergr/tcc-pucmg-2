# Referências para relatório final

## A ler

* [Boa introdução a LDA - https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158)
* [Excelente tópico sobre semelhança entre documentos com LDA](https://www.kaggle.com/ktattan/lda-and-document-similarity)
* Número de tópicos
    * <https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/>
    * <https://stackoverflow.com/questions/17421887/how-to-determine-the-number-of-topics-for-lda>
    * <https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html>
* Coherence score
    * <http://qpleple.com/perplexity-to-evaluate-topic-models/>
    * <https://stats.stackexchange.com/questions/375062/how-does-topic-coherence-score-in-lda-intuitively-makes-sense>
    * <https://radimrehurek.com/gensim/models/coherencemodel.html>
    * <https://ieeexplore.ieee.org/document/8259775>
    * <https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html>
    * <https://towardsdatascience.com/stemming-lemmatization-what-ba782b7c0bd8>

## Rever devido a complexidade

## Lido

* <https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0>
    * Contém exemplos de bigram e trigram
