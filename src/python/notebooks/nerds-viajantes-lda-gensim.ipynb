{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modulos da propria aplicacao\n",
    "from limpeza import limpeza_posts, limpeza_texto\n",
    "from repository import nerds_viajantes, wikipedia, mongo_utils\n",
    "from treinamento import dicionario, treinamento_lda\n",
    "from treinamento.treinamento_lda import TreinamentoLda\n",
    "\n",
    "from repository.mongo_utils import get_pages_content_collection\n",
    "from repository.wikipedia import WikipediaRepo\n",
    "\n",
    "wikipedia_repo = WikipediaRepo(collection=get_pages_content_collection())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity.similarity import SimilarityCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos de pacotes\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalho com base de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar posts\n",
    "published = nerds_viajantes.read_published()\n",
    "\n",
    "# Remove posts desnecessarios\n",
    "posts = limpeza_posts.limpar_posts(published)\n",
    "\n",
    "# Limpa texto e gera documentos para treinamento\n",
    "documentos = posts['content'].apply(limpeza_texto.limpar_texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de modelo para posts do Nerds Viajantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajustando modelo com 100 topicos e 2 passes\n"
     ]
    }
   ],
   "source": [
    "# Ajusta modelo LDA para os posts do nerds viajantes. O resultado gerado contem o dicionario, o corpus de dados e o modelo gerado\n",
    "treinamento_lda = TreinamentoLda(num_topics=100, passes=2)\n",
    "lda_nerds_viajantes = treinamento_lda.ajustar_modelo(documentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicao de probabilidades de topicos para posts do Nerds Viajantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378, 100)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilidades_topicos_nerds_viajantes = [treinamento_lda.calcular_probabilidades_documento(dnv, lda_nerds_viajantes) for dnv in documentos]\n",
    "np.array(probabilidades_topicos_nerds_viajantes).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste com classe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalho com base da Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = wikipedia_repo.find_all()\n",
    "pages_df = pd.DataFrame(pages)\n",
    "pages_df = pages_df.drop(['_id', 'ns', 'type', 'download', 'categories', 'wikitext', 'text'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpa texto e gera documentos para treinamento\n",
    "documentos_wikipedia = pages_df['text_clean'].apply(limpeza_texto.limpar_texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicao de topicos para documentos da Wikipedia\n",
    "\n",
    "- Quantidade de documentos: 2252\n",
    "- Indice do maior documento: 123\n",
    "- Tamanho do maior documento: 10018 tokens\n",
    "- Tempo gasto para definir tópicos de maior documento: 0.02167201042175293 segundos\n",
    "- Tempo estimado para todos os documentos (com base no tempo do maior): 18.83 segundos\n",
    "    * Na verdade este valor variou muito, não é fixo, mas a maioria das vezes ficou em torno de 20 segundos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def avaliar_tempos_definicao_topicos_wikipedia():\n",
    "    print(f'Quantidade de documentos: {len(documentos_wikipedia)}')\n",
    "\n",
    "    tamanhos = [len(d) for d in documentos_wikipedia]\n",
    "    indice_maior_documento = np.argmax(tamanhos)\n",
    "    print(f'Indice do maior documento: {indice_maior_documento}')\n",
    "    print(f'Tamanho do maior documento: {tamanhos[indice_maior_documento]} tokens')\n",
    "\n",
    "    maior_documento = documentos_wikipedia[indice_maior_documento]\n",
    "    start_time = time.time()\n",
    "    treinamento_lda.calcular_probabilidades_documento(maior_documento, lda_nerds_viajantes)\n",
    "    tempo_gasto_maior_documento = time.time() - start_time\n",
    "    print(f\"Tempo gasto na definicao dos topicos do maior documento: {tempo_gasto_maior_documento:.2f} segundos\")\n",
    "    print(f'Tempo estimado para todos documentos: {len(documentos_wikipedia) * tempo_gasto_maior_documento:.2f} segundos')\n",
    "\n",
    "# avaliar_tempos_definicao_topicos_wikipedia()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo de probabilidade de topicos para paginas da wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo total gasto na definicao dos topicos: 7.56 segundos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "probabilidades_topicos_wikipedia = [\n",
    "    treinamento_lda.calcular_probabilidades_documento(dw, lda_nerds_viajantes) for dw in documentos_wikipedia]\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Tempo total gasto na definicao dos topicos: {elapsed_time:.2f} segundos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculo de semelhanca entre posts do Nerds Viajantes e Posts da Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de posts do nerds viajantes: 378\n",
      "Tamanho do array de probabilidades de posts do nerds viajantes: 100\n",
      "Quantidade de paginas da wikipedia: 2252\n",
      "Tamanho do array de probabilidades de paginas da wikipedia: 100\n"
     ]
    }
   ],
   "source": [
    "print(f'Quantidade de posts do nerds viajantes: {len(probabilidades_topicos_nerds_viajantes)}')\n",
    "print(f'Tamanho do array de probabilidades de posts do nerds viajantes: {len(probabilidades_topicos_nerds_viajantes[0])}')\n",
    "print(f'Quantidade de paginas da wikipedia: {len(probabilidades_topicos_wikipedia)}')\n",
    "print(f'Tamanho do array de probabilidades de paginas da wikipedia: {len(probabilidades_topicos_wikipedia[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema encontrado\n",
    "\n",
    "Ao calcular as probabilidades que determinados documentos estejam em determinados tópicos vimos que poucos tópicos determinam um documento, independente de usarmos 13 ou 100 tópicos no modelo. Para calcular a matrix esparsa das similaridades eu acabei tendo que preencher um array com 0 em quase todas as posições para que não desse erro.\n",
    "Devemos estudar para ver se o modelo LDA é realmente a melhor abordagem neste caso ou se não temos muitos poucos documentos.\n",
    "\n",
    "Sem preencher com 0's os tópicos que não contribuiam para cada documento o que acontecia era que cada array de probabilidade por tópico de um documento tinha um tamanho diferente, o que dava erro ao criar a matrix esparsa, que espera que todos tenham a mesma quantidade de colunas, que no caso é a quantidade de tópicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_calculator = SimilarityCalculator(probabilidades_topicos_wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcula as paginas da wikipedia mais parecidas com os posts do nerds viajantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_mais_parecidos = [similarity_calculator.get_most_similar_documents(pnv) for pnv in probabilidades_topicos_nerds_viajantes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uma análise de post específico (bariloche-cerro-tronador-e-cascada-los-alerces)\n",
    "\n",
    "Post escolhido do Nerds viajantes para analisar:\n",
    "\n",
    "- id: 1356\n",
    "- indice no array: 49\n",
    "- name: bariloche-cerro-tronador-e-cascada-los-alerces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name       bariloche-cerro-tronador-e-cascada-los-alerces\n",
       "title    Bariloche - Cerro Tronador e Cascada los Alerces\n",
       "Name: 49, dtype: object"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indice_post_nv = 49\n",
    "posts.iloc[49][['name', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagina da wikipedia: Geografia da Argentina\n"
     ]
    }
   ],
   "source": [
    "indice_wikipedia = 1145\n",
    "print(f'Pagina da wikipedia: {pages_df.iloc[0].title}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia mais parecido:                                               title         country\n",
      "1119                                    Rio Awatere   Nova Zelandia\n",
      "2185                                     Rio Innoko  Estados Unidos\n",
      "1109                                      Rio Arrow   Nova Zelandia\n",
      "1194                                 Albion (Texas)  Estados Unidos\n",
      "1258                                 Boston (Texas)  Estados Unidos\n",
      "1789  Parque Nacional e Reserva Gates of the Arctic  Estados Unidos\n",
      "1124                                    Rio Barrier   Nova Zelandia\n",
      "894                   Lista de atrações de Auckland   Nova Zelandia\n",
      "2188                                    Rio Nowitna  Estados Unidos\n",
      "1138                                   Rio Turakina   Nova Zelandia\n"
     ]
    }
   ],
   "source": [
    "paginas_wikipedia_mais_parecidas = wikipedia_mais_parecidos[indice_post_nv]\n",
    "print(f'Wikipedia mais parecido: {pages_df.iloc[paginas_wikipedia_mais_parecidas, [1, 2]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comentários sobre o resultado e trabalho futuro**\n",
    "\n",
    "- Todos os posts que vem como os mais parecidos são relacionados a rios\n",
    "- Os rios dos resultados são distribuídos entre os países Estados Unidos e Nova Zelândia mas curiosamente não Argentina\n",
    "    * Provavelmente eu não baixei os posts de rios da Argentina\n",
    "\n",
    "**Trabalho futuro**\n",
    "\n",
    "- Analisar qual tópico mais aparece para este post\n",
    "    * Analisar também do mais parecido na wikipedia\n",
    "- Analisar palavras que mais contribuem para os tópicos em questão"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e40f5302c6d4b9de2141353283620edefcabae1dbe23ed7117377b1c1ce32e40"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('tcc-venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}