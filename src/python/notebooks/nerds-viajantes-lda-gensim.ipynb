{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento Nerds Viajantes - LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modulos da propria aplicacao\n",
    "from limpeza import limpeza_posts, limpeza_texto\n",
    "from repository import nerds_viajantes, wikipedia, mongo_utils\n",
    "from treinamento import dicionario, treinamento_lda\n",
    "from treinamento.treinamento_lda import TreinamentoLda\n",
    "\n",
    "from repository.mongo_utils import get_pages_content_collection\n",
    "from repository.wikipedia import WikipediaRepo\n",
    "\n",
    "wikipedia_repo = WikipediaRepo(collection=get_pages_content_collection())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity.similarity import SimilarityCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos de pacotes\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalho com base de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_documentos_nerds_viajantes():\n",
    "    # Carregar posts\n",
    "    published = nerds_viajantes.read_published()\n",
    "\n",
    "    # Remove posts desnecessarios\n",
    "    posts = limpeza_posts.limpar_posts(published)\n",
    "\n",
    "    # Limpa texto e gera documentos para treinamento\n",
    "    documentos = posts['content'].apply(limpeza_texto.limpar_texto)\n",
    "\n",
    "    return documentos\n",
    "\n",
    "documentos = preparar_documentos_nerds_viajantes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de modelo para posts do Nerds Viajantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajusta modelo LDA para os posts do nerds viajantes. O resultado gerado contem o dicionario, o corpus de dados e o modelo gerado\n",
    "treinamento_lda = TreinamentoLda()\n",
    "lda_nerds_viajantes = treinamento_lda.ajustar_modelo(documentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicao de probabilidades de topicos para posts do Nerds Viajantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilidades_topicos_nerds_viajantes = [treinamento_lda.calcular_probabilidades_documento(dnv, lda_nerds_viajantes) for dnv in documentos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste com classe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalho com base da Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = wikipedia_repo.find_all()\n",
    "pages_df = pd.DataFrame(pages)\n",
    "pages_df = pages_df.drop(['_id', 'ns', 'type', 'download', 'categories', 'wikitext', 'text'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [geograf, argentin, domíni, estud, conhec, sob...\n",
       "1    [banc, namuncur, banc, burdwood, plataform, su...\n",
       "Name: text_clean, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limpa texto e gera documentos para treinamento\n",
    "documentos_wikipedia = pages_df['text_clean'].apply(limpeza_texto.limpar_texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicao de topicos para documentos da Wikipedia\n",
    "\n",
    "- Quantidade de documentos: 2252\n",
    "- Indice do maior documento: 123\n",
    "- Tamanho do maior documento: 10018 tokens\n",
    "- Tempo gasto para definir tópicos de maior documento: 0.02167201042175293 segundos\n",
    "- Tempo estimado para todos os documentos (com base no tempo do maior): 18.83 segundos\n",
    "    * Na verdade este valor variou muito, não é fixo, mas a maioria das vezes ficou em torno de 20 segundos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def avaliar_tempos_definicao_topicos_wikipedia():\n",
    "    print(f'Quantidade de documentos: {len(documentos_wikipedia)}')\n",
    "\n",
    "    tamanhos = [len(d) for d in documentos_wikipedia]\n",
    "    indice_maior_documento = np.argmax(tamanhos)\n",
    "    print(f'Indice do maior documento: {indice_maior_documento}')\n",
    "    print(f'Tamanho do maior documento: {tamanhos[indice_maior_documento]} tokens')\n",
    "\n",
    "    maior_documento = documentos_wikipedia[indice_maior_documento]\n",
    "    start_time = time.time()\n",
    "    treinamento_lda.calcular_probabilidades_documento(maior_documento, lda_nerds_viajantes)\n",
    "    tempo_gasto_maior_documento = time.time() - start_time\n",
    "    print(f\"Tempo gasto na definicao dos topicos do maior documento: {tempo_gasto_maior_documento:.2f} segundos\")\n",
    "    print(f'Tempo estimado para todos documentos: {len(documentos_wikipedia) * tempo_gasto_maior_documento:.2f} segundos')\n",
    "\n",
    "# avaliar_tempos_definicao_topicos_wikipedia()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo de probabilidade de topicos para paginas da wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo total gasto na definicao dos topicos: 1.46 segundos\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "probabilidades_topicos_wikipedia = [\n",
    "    treinamento_lda.calcular_probabilidades_documento(dw, lda_nerds_viajantes) for dw in documentos_wikipedia]\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Tempo total gasto na definicao dos topicos: {elapsed_time:.2f} segundos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculo de semelhanca entre posts do Nerds Viajantes e Posts da Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de posts do nerds viajantes: 378\n",
      "Tamanho do array de probabilidades de posts do nerds viajantes: 13\n",
      "Quantidade de paginas da wikipedia: 2252\n",
      "Tamanho do array de probabilidades de paginas da wikipedia: 13\n"
     ]
    }
   ],
   "source": [
    "print(f'Quantidade de posts do nerds viajantes: {len(probabilidades_topicos_nerds_viajantes)}')\n",
    "print(f'Tamanho do array de probabilidades de posts do nerds viajantes: {len(probabilidades_topicos_nerds_viajantes[0])}')\n",
    "print(f'Quantidade de paginas da wikipedia: {len(probabilidades_topicos_wikipedia)}')\n",
    "print(f'Tamanho do array de probabilidades de paginas da wikipedia: {len(probabilidades_topicos_wikipedia[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helder/estudos/tcc-pucmg-2/src/python/notebooks/similarity/similarity.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in self.modelo_lda[self.corpus]])\n"
     ]
    }
   ],
   "source": [
    "similarity_calculator = SimilarityCalculator(lda_nerds_viajantes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<similarity.similarity.SimilarityCalculator object at 0x7f03bc0b77f0>\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e40f5302c6d4b9de2141353283620edefcabae1dbe23ed7117377b1c1ce32e40"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('tcc-venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}