{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploração - Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from repository.mongo_utils import get_pages_content_collection\n",
    "from repository.wikipedia import WikipediaRepo\n",
    "\n",
    "wikipedia_repo = WikipediaRepo(collection=get_pages_content_collection())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obter todas as páginsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = wikipedia_repo.find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_df = pd.DataFrame(pages)\n",
    "pages_df = pages_df.drop(['_id', 'ns', 'type', 'download', 'categories', 'wikitext'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contagem de posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foram encontradas 2252 páginas com contéudo\n"
     ]
    }
   ],
   "source": [
    "print(f'Foram encontradas {len(pages_df)} páginas com contéudo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploração antes da limpeza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tamanho médio dos documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lengths = pages_df['text'].apply(len)\n",
    "pages_df['raw_length'] = text_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas considerações\n",
    "\n",
    "- Tamanho mínimo de 799 incluindo tags html indica que pode haver documentos muito pequenos\n",
    "- Desvio padrão maior que a média indica que pode haver uma variação muito grande nos tamanhos dos documentos\n",
    "    * Verificar quantiles para ver a variação por quartis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "analisar_tamanhos = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_lengths(lenghts):\n",
    "    print(\"Measures of Central Tendency\")\n",
    "    print(\"Mean =\", np.mean(lenghts))\n",
    "    print(\"Median =\", np.median(lenghts))\n",
    "    print(\"Measures of Dispersion\")\n",
    "    print(\"Minimum =\", np.amin(lenghts))\n",
    "    print(\"Maximum =\", np.amax(lenghts))\n",
    "    print(\"Range =\", np.ptp(lenghts))\n",
    "    print(\"Varience =\", np.var(lenghts))\n",
    "    print(\"Standard Deviation =\", np.std(lenghts))\n",
    "\n",
    "if analisar_tamanhos:\n",
    "    describe_lengths(text_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analisar_tamanhos:\n",
    "    np.quantile(text_lengths, [0, 0.25, 0.5, 0.75, 0.9, 0.95, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificar distribuição de tamanhos de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analisar_tamanhos:\n",
    "    sns.displot(pages_df, x='raw_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisar documentos acima de 100000 caracteres\n",
    "if analisar_tamanhos:\n",
    "    pages_df_acima_100k = pages_df[pages_df['raw_length'] >= 100000]\n",
    "    pages_df_acima_100k.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from limpeza.limpeza_texto import remover_html_tags\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pages_df['text'].values\n",
    "texto = texts[0] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primeira  tentativa - removendo elementos individualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_estilos_partes(texto):\n",
    "    t = remover_html_tags(texto)\n",
    "    t = re.sub('body\\.skin\\-[a-z]+ ', '', t)\n",
    "    t = re.sub('.mw\\-parser-[a-z]+ ', '', t)\n",
    "    t = re.sub('t[hd]\\.mbox\\-[a-z]+[ ,]', '', t)\n",
    "    t = re.sub('table\\.[acfiot]mbox,', '', t)\n",
    "    t = re.sub('table\\.[acfiot]mbox\\-[a-z]+,', '', t)\n",
    "    t = re.sub('\\.[ait]mbox[ ,]', '', t)\n",
    "    t = re.sub('\\.mbox\\-[a-z]+[ ,]', '', t)\n",
    "    t = re.sub('\\.tmbox\\.mbox\\-[a-z]+[ ,]', '', t)\n",
    "    t = re.sub('\\.mediawiki ', '', t)\n",
    "    t = re.sub('\\.compact-ambox[ ,]', '', t)\n",
    "\n",
    "    print(f'Tamanho do texto : {len(t)}/{len(texto)}')\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segunda tentativa\n",
    "\n",
    "- Peguei o conteúdo html, colei em um formatador externo e decidi tomar as seguintes ações\n",
    "    * Remover tag ```<scripts>``` e todo seu conteúdo interno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando segundo texto, que tem tamanho 33027, mais perto da media\n",
    "# [len(t) for t in texts[:20]]\n",
    "\n",
    "texto = texts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_estilos_partes_2(texto):\n",
    "    t = re.sub('<style .*?style>', '', texto) # Removendo tag de estilos\n",
    "    print(f'Tamanho do texto apos remocao de estilos: {len(t)}/{len(texto)}. Aproximadamente {(len(t)/len(texto)) * 100:.2f}% mantido')\n",
    "\n",
    "    t = re.sub('\\n', '', t)\n",
    "    t = re.sub('<table class=\"box-Sem_fontes.*?/table>', '', t)\n",
    "    t = re.sub('<!--.*?-->', '', t) # Remover comentarios\n",
    "    print(f'Tamanho do texto apos remocao conteudo: {len(t)}/{len(texto)}. Aproximadamente {(len(t)/len(texto)) * 100:.2f}% mantido')\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de parágrafos\n",
    "\n",
    "Após conseguir remover um pouco de texto na tentativa 2 anterior vamos extrair apenas textos de parágrafos e ver como fica o resultado. Um risco nesta abordagem é tirar conteúdos interessantes de tabelas ou listas mas deixar este problema de lado por enquanto.\n",
    "\n",
    "- **Vou ignorar inclusive a limpeza de texto feito previamente e ir direto aos parágrafos.**\n",
    "- **Avaliar possibilidade de pegar conteúdo <pre></pre>.**\n",
    "- Uma análise posterior de tamanho dos documentos deve ser feita\n",
    "- Deve-se remover tags html de dentro dos parágrafos pois estes podem conter elementos como links ou spans\n",
    "    * **Isto é bem resolvido pelo método getText() do BeautifulSoup quando aplicado ao elemento p**\n",
    "- **Esta foi a solução final adotada e a extração de texto puro foi feita no método *extrair_textos_puros* da classe *WikipediaCleaner* do pacote *limpeza.wikipedia***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "executar_extracao_analise_paragrafos = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da primeira vez que executei o extrator de textos das páginas html ele demorou 38.5 segundos para extrair o conteudo de todas as páginas. Considerar se não vale a pena extrair e guardar este conteúdo MongoDB a partir do momento em que a extração já estiver definida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texta(raw_text):\n",
    "    soup = BeautifulSoup(raw_text, 'html.parser')\n",
    "    ps = soup.find_all('p')\n",
    "    pres = soup.find_all('pre')\n",
    "\n",
    "    ps_text = ' '.join([p.getText() for p in ps])\n",
    "    pres_text = ' '.join([pre.getText() for pre in pres])\n",
    "\n",
    "    return (ps_text, pres_text)\n",
    "\n",
    "if executar_extracao_analise_paragrafos:\n",
    "    extracted = [extract_texta(t) for t in texts]\n",
    "    tamanhos_pres = [len(ex[1]) for ex in extracted]\n",
    "    tamanho_df = pd.DataFrame(tamanhos_pres, columns=['tamanhopre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if executar_extracao_analise_paragrafos:\n",
    "    sns.histplot(tamanho_df, x='tamanhopre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if executar_extracao_analise_paragrafos:\n",
    "    tamanho_df['tamanhopre'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if executar_extracao_analise_paragrafos:\n",
    "    # Apenas 3 páginas contém texto pre\n",
    "    existe_pre = [ex[1] for ex in extracted if len(ex[1]) > 0]\n",
    "    len(existe_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if executar_extracao_analise_paragrafos:\n",
    "    textos_completos = [ex[0] + ' ' + ex[1] if len(ex[1]) > 0 else ex[0] for ex in extracted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if executar_extracao_analise_paragrafos:\n",
    "    tamanhos_textos_completos = [len(t) for t in textos_completos]\n",
    "    np.quantile(tamanhos_textos_completos, q=[0, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if executar_extracao_analise_paragrafos:\n",
    "    # Breaks texts in words to count words\n",
    "    texts_words = [t.split(' ') for t in textos_completos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if executar_extracao_analise_paragrafos:\n",
    "    tamanhos_em_palavras = [len(tw) for tw in texts_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if executar_extracao_analise_paragrafos:\n",
    "    # The first command is to suppress the scientific notation\n",
    "    np.set_printoptions(suppress=True)\n",
    "    np.quantile(tamanhos_em_palavras, q=[0, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise do tamanho de palavras\n",
    "\n",
    "- [https://towardsdatascience.com/short-text-topic-modeling-70e50a57c883](https://towardsdatascience.com/short-text-topic-modeling-70e50a57c883)\n",
    "- [https://www.researchgate.net/post/What-would-be-considered-the-least-number-of-documents-for-training-an-LDA-SLDA-topic-model-Is-a-corpus-of-200-documents-large-enough](https://www.researchgate.net/post/What-would-be-considered-the-least-number-of-documents-for-training-an-LDA-SLDA-topic-model-Is-a-corpus-of-200-documents-large-enough)\n",
    "- [https://stackoverflow.com/questions/36673316/latent-dirichlet-allocationlda-performance-by-limiting-word-size-for-corpus-do](https://stackoverflow.com/questions/36673316/latent-dirichlet-allocationlda-performance-by-limiting-word-size-for-corpus-do)\n",
    "- [https://www.machinelearningplus.com/topic-modeling-visualization-how-to-present-results-lda-models/](https://www.machinelearningplus.com/topic-modeling-visualization-how-to-present-results-lda-models/)\n",
    "- [https://www.frontiersin.org/articles/10.3389/frai.2020.00042/full](https://www.frontiersin.org/articles/10.3389/frai.2020.00042/full)\n",
    "- [https://arxiv.org/pdf/1904.07695.pdf](https://arxiv.org/pdf/1904.07695.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza de conteúdo html (extrair apenas texto)\n",
    "\n",
    "- Obter todas as páginas da collection *pages_content*\n",
    "- Para cada página, utilizando campo *text*\n",
    "    - Extrair apenas o texto puro\n",
    "    - Gravar atrituto text_clean\n",
    "- Foi implementado para isto o método *extrair_textos_puros* da classe *WikipediaCleaner* do pacote *limpeza.wikipedia*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise tamanhos textos\n",
    "\n",
    "- **Com base nos textos puros extraídos dos htmls das páginas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "executar_extracao_analise_paragrafos = True\n",
    "\n",
    "if executar_extracao_analise_paragrafos:\n",
    "    pages = wikipedia_repo.find_all()\n",
    "    textos_puros = [page['text_clean'] for page in pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     1.  ,    129.2 ,    262.  ,    481.  ,   1016.25,   2348.7 ,\n",
       "         4102.8 , 106407.  ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analise de tamanho dos textos completos\n",
    "if executar_extracao_analise_paragrafos:\n",
    "    np.set_printoptions(suppress=True)\n",
    "    tamanhos_textos_completos = [len(t) for t in textos_puros]\n",
    "    np.quantile(tamanhos_textos_completos, q=[0, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analise da quantidade de palavras dos textos completos\n",
    "if executar_extracao_analise_paragrafos:\n",
    "    # Breaks texts in words to count words\n",
    "    texts_words = [t.split(' ') for t in textos_puros]\n",
    "    tamanhos_em_palavras = [len(tw) for tw in texts_words]\n",
    "\n",
    "    # The first command is to suppress the scientific notation\n",
    "    np.set_printoptions(suppress=True)\n",
    "    print(np.quantile(tamanhos_em_palavras, q=[0, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 1]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e40f5302c6d4b9de2141353283620edefcabae1dbe23ed7117377b1c1ce32e40"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('tcc-venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}