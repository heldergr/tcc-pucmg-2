{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exploração - Wikipedia"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from repository.mongo_utils import get_pages_content_collection, get_wikipedia_collection\n",
    "from repository.wikipedia import WikipediaRepo\n",
    "\n",
    "wikipedia_repo = WikipediaRepo(collection=get_pages_content_collection())\n",
    "wikipedia_repo_brasil = WikipediaRepo(collection=get_wikipedia_collection('pages_content_brasil'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Obter todas as páginsa"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "pages = wikipedia_repo.find_all() + wikipedia_repo_brasil.find_all()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from fonte_dados.wikipedia import Wikipedia\n",
    "wp = Wikipedia()\n",
    "fdw = wp.carregar_dados()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "pages_df = pd.DataFrame(pages)\n",
    "pages_df = pages_df.drop(['_id', 'ns', 'type', 'download', 'categories', 'wikitext'], axis='columns')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Contagem de posts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(f'Foram encontradas {len(pages_df)} páginas com contéudo')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Foram encontradas 3016 páginas com contéudo\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploração antes da limpeza"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tamanho médio dos documentos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "text_lengths = pages_df['text'].apply(len)\n",
    "pages_df['raw_length'] = text_lengths"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Algumas considerações\n",
    "\n",
    "- Tamanho mínimo de 799 incluindo tags html indica que pode haver documentos muito pequenos\n",
    "- Desvio padrão maior que a média indica que pode haver uma variação muito grande nos tamanhos dos documentos\n",
    "    * Verificar quantiles para ver a variação por quartis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "analisar_tamanhos = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def describe_lengths(lenghts):\n",
    "    print(\"Measures of Central Tendency\")\n",
    "    print(\"Mean =\", np.mean(lenghts))\n",
    "    print(\"Median =\", np.median(lenghts))\n",
    "    print(\"Measures of Dispersion\")\n",
    "    print(\"Minimum =\", np.amin(lenghts))\n",
    "    print(\"Maximum =\", np.amax(lenghts))\n",
    "    print(\"Range =\", np.ptp(lenghts))\n",
    "    print(\"Varience =\", np.var(lenghts))\n",
    "    print(\"Standard Deviation =\", np.std(lenghts))\n",
    "\n",
    "if analisar_tamanhos:\n",
    "    describe_lengths(text_lengths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "if analisar_tamanhos:\n",
    "    np.quantile(text_lengths, [0, 0.25, 0.5, 0.75, 0.9, 0.95, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Verificar distribuição de tamanhos de documentos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "if analisar_tamanhos:\n",
    "    sns.displot(pages_df, x='raw_length')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Analisar documentos acima de 100000 caracteres\n",
    "if analisar_tamanhos:\n",
    "    pages_df_acima_100k = pages_df[pages_df['raw_length'] >= 100000]\n",
    "    pages_df_acima_100k.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Limpeza de dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from limpeza.limpeza_texto import remover_html_tags\n",
    "\n",
    "import re"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "texts = pages_df['text'].values\n",
    "texto = texts[0] \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extração de parágrafos\n",
    "\n",
    "Após conseguir remover um pouco de texto na tentativa 2 anterior vamos extrair apenas textos de parágrafos e ver como fica o resultado. Um risco nesta abordagem é tirar conteúdos interessantes de tabelas ou listas mas deixar este problema de lado por enquanto.\n",
    "\n",
    "- **Vou ignorar inclusive a limpeza de texto feito previamente e ir direto aos parágrafos.**\n",
    "- **Avaliar possibilidade de pegar conteúdo <pre></pre>.**\n",
    "- Uma análise posterior de tamanho dos documentos deve ser feita\n",
    "- Deve-se remover tags html de dentro dos parágrafos pois estes podem conter elementos como links ou spans\n",
    "    * **Isto é bem resolvido pelo método getText() do BeautifulSoup quando aplicado ao elemento p**\n",
    "- **Esta foi a solução final adotada e a extração de texto puro foi feita no método *extrair_textos_puros* da classe *WikipediaCleaner* do pacote *limpeza.wikipedia***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "executar_extracao_analise_paragrafos = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Da primeira vez que executei o extrator de textos das páginas html ele demorou 38.5 segundos para extrair o conteudo de todas as páginas. Considerar se não vale a pena extrair e guardar este conteúdo MongoDB a partir do momento em que a extração já estiver definida."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def extract_texta(raw_text):\n",
    "    soup = BeautifulSoup(raw_text, 'html.parser')\n",
    "    ps = soup.find_all('p')\n",
    "    pres = soup.find_all('pre')\n",
    "\n",
    "    ps_text = ' '.join([p.getText() for p in ps])\n",
    "    pres_text = ' '.join([pre.getText() for pre in pres])\n",
    "\n",
    "    return (ps_text, pres_text)\n",
    "\n",
    "if executar_extracao_analise_paragrafos:\n",
    "    extracted = [extract_texta(t) for t in texts]\n",
    "    tamanhos_pres = [len(ex[1]) for ex in extracted]\n",
    "    tamanho_df = pd.DataFrame(tamanhos_pres, columns=['tamanhopre'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "if executar_extracao_analise_paragrafos:\n",
    "    sns.histplot(tamanho_df, x='tamanhopre')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "if executar_extracao_analise_paragrafos:\n",
    "    tamanho_df['tamanhopre'].describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "if executar_extracao_analise_paragrafos:\n",
    "    # Apenas 3 páginas contém texto pre\n",
    "    existe_pre = [ex[1] for ex in extracted if len(ex[1]) > 0]\n",
    "    len(existe_pre)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "if executar_extracao_analise_paragrafos:\n",
    "    textos_completos = [ex[0] + ' ' + ex[1] if len(ex[1]) > 0 else ex[0] for ex in extracted]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "if executar_extracao_analise_paragrafos:\n",
    "    tamanhos_textos_completos = [len(t) for t in textos_completos]\n",
    "    np.quantile(tamanhos_textos_completos, q=[0, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "if executar_extracao_analise_paragrafos:\n",
    "    # Breaks texts in words to count words\n",
    "    texts_words = [t.split(' ') for t in textos_completos]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "if executar_extracao_analise_paragrafos:\n",
    "    tamanhos_em_palavras = [len(tw) for tw in texts_words]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "if executar_extracao_analise_paragrafos:\n",
    "    # The first command is to suppress the scientific notation\n",
    "    np.set_printoptions(suppress=True)\n",
    "    np.quantile(tamanhos_em_palavras, q=[0, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Análise do tamanho de palavras\n",
    "\n",
    "- [https://www.frontiersin.org/articles/10.3389/frai.2020.00042/full](https://www.frontiersin.org/articles/10.3389/frai.2020.00042/full)\n",
    "- [https://arxiv.org/pdf/1904.07695.pdf](https://arxiv.org/pdf/1904.07695.pdf)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Limpeza de conteúdo html (extrair apenas texto)\n",
    "\n",
    "- Obter todas as páginas da collection *pages_content*\n",
    "- Para cada página, utilizando campo *text*\n",
    "    - Extrair apenas o texto puro\n",
    "    - Gravar atrituto text_clean\n",
    "- Foi implementado para isto o método *extrair_textos_puros* da classe *WikipediaCleaner* do pacote *limpeza.wikipedia*."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Análise tamanhos textos\n",
    "\n",
    "- **Com base nos textos puros extraídos dos htmls das páginas**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "executar_extracao_analise_paragrafos = False\n",
    "\n",
    "if executar_extracao_analise_paragrafos:\n",
    "    pages = wikipedia_repo.find_all()\n",
    "    textos_puros = [page['text_clean'] for page in pages]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# Analise de tamanho dos textos completos\n",
    "if executar_extracao_analise_paragrafos:\n",
    "    np.set_printoptions(suppress=True)\n",
    "    tamanhos_textos_completos = [len(t) for t in textos_puros]\n",
    "    np.quantile(tamanhos_textos_completos, q=[0, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# Analise da quantidade de palavras dos textos completos\n",
    "if executar_extracao_analise_paragrafos:\n",
    "    # Breaks texts in words to count words\n",
    "    texts_words = [t.split(' ') for t in textos_puros]\n",
    "    tamanhos_em_palavras = [len(tw) for tw in texts_words]\n",
    "\n",
    "    # The first command is to suppress the scientific notation\n",
    "    np.set_printoptions(suppress=True)\n",
    "    print(np.quantile(tamanhos_em_palavras, q=[0, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 1]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Limpeza de textos puros"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "from limpeza import limpeza_texto\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "textos_puros = pages_df['text_clean'].values\n",
    "deve_limpar_textos_puros = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def limpar_texto_puro(texto_puro, remover_pontuacoes=True, remover_stopwords=True):\n",
    "    texto_puro_limpo = texto_puro\n",
    "    if remover_pontuacoes:\n",
    "        texto_puro_limpo = limpeza_texto.remover_pontuacoes(texto_puro_limpo)\n",
    "\n",
    "    tokens = word_tokenize(texto_puro_limpo)\n",
    "    if remover_stopwords:\n",
    "        tokens = limpeza_texto.remover_stopwords(tokens)\n",
    "\n",
    "    tokens = limpeza_texto.stemm_text(tokens)\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "textos_puros_limpos = []\n",
    "if deve_limpar_textos_puros:\n",
    "    # Na primeira execucao para todos os textos a execucao completa demorou cerca de 12 segundos\n",
    "    textos_puros_limpos = [limpar_texto_puro(texto_puro) for texto_puro in textos_puros]"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'limpeza.limpeza_texto' has no attribute 'remove_stopwords'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-07d2a248a43a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdeve_limpar_textos_puros\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Na primeira execucao para todos os textos a execucao completa demorou cerca de 12 segundos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtextos_puros_limpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlimpar_texto_puro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto_puro\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtexto_puro\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtextos_puros\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-07d2a248a43a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdeve_limpar_textos_puros\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Na primeira execucao para todos os textos a execucao completa demorou cerca de 12 segundos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtextos_puros_limpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlimpar_texto_puro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto_puro\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtexto_puro\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtextos_puros\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-07d2a248a43a>\u001b[0m in \u001b[0;36mlimpar_texto_puro\u001b[0;34m(texto_puro, remover_pontuacoes, remover_stopwords)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto_puro_limpo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mremover_stopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlimpeza_texto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlimpeza_texto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemm_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'limpeza.limpeza_texto' has no attribute 'remove_stopwords'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Nuvem de palavras"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_text = ' '.join(textos_puros_limpos)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stop_words = set(STOPWORDS)\n",
    "stop_words.update(['país', 'coorden', 'algum', 'desd', 'sobr', 'dev', 'exist', 'part', 'cidad', 'mai'])\n",
    "\n",
    "cloud = WordCloud(max_words=100, background_color='white', stopwords=stop_words).generate(all_text)\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e40f5302c6d4b9de2141353283620edefcabae1dbe23ed7117377b1c1ce32e40"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('tcc-venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}